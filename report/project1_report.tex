\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{amsmath}
\hypersetup{
	colorlinks,
	linkcolor={blue},
	citecolor={blue},
	urlcolor={blue}
}
\usepackage{graphicx}	% For figure environment
\usepackage[utf8]{inputenc}

\begin{document}
\title{Learning to discover: the Higgs boson machine learning challenge}

\author{\IEEEauthorblockN{Oriol Barbany\IEEEauthorrefmark{1},
		Natalia Gullon\IEEEauthorrefmark{2} and Sophia Kypraiou\IEEEauthorrefmark{3}}
	\IEEEauthorblockA{Machine Learning (CS-433), School of Computer and Communication Sciences \\ 
		École Polytechnique Fédérale de Lausanne\\
		Email: \IEEEauthorrefmark{1}\href{mailto:oriol.barbanymayor@epfl.ch}{oriol.barbanymayor@epfl.ch},
		\IEEEauthorrefmark{2}\href{mailto:natalia.gullonaltes@epfl.ch}{natalia.gullonaltes@epfl.ch},
		\IEEEauthorrefmark{3}\href{mailto:sofia.kypraiou@epfl.ch}{sofia.kypraiou@epfl.ch}}}

\maketitle

\begin{abstract}
  Machine learning techniques have been successfully applied to a bunch of problems from different disciplines. In most cases where data is too complex or too large to be treated, statistical models are needed and their results yield to the state of the art. This paper introduces a machine learning method to predict if a certain event corresponds to the decayment of the Higgs boson given features extracted from the ATLAS experiment performed at the Large Hadron Collider at CERN. The proposed algorithm is based on regularized logistic regression with adaptive learning rate, and obtained a categorization accuracy of 0.82 in the Kaggle platform of the challenge\footnote{\url{https://www.kaggle.com/c/epfml18-higgs}}.
\end{abstract}

%\begin{IEEEkeywords}
%    Machine Learning, Higgs boson, Logistic regression
%\end{IEEEkeywords}

\section{Introduction}

The Higgs boson is an elementary particle in the Standard Model of physics that explains why other particles have mass. In order to discover this particle, protons are smashed one into another at high speeds to generate smaller particles as by-products of collisions. Decay signature and other products that result from its decay process are measured and the likelihood that a given event's signature is the result of a Higgs boson is then estimated.

% The results of its decay process such as decay signatures and other products are measured, and the 

Therefore, the aim of this project is to build a binary classifier to predict whether an event corresponds to the decayment of a Higgs boson or not from a given vector of features representing the decay signature of a collision event.


\section{Methodology}
\label{sec:methodology}
\subsection{Data cleaning}
\label{sec:data-cleaning}
Most of the time spent on this project is dedicated to the preprocessing of the raw data. The features of the different events corresponding to experiments performed at ATLAS have a lot of meaning-less values, meaning that the obtained results are outside the normal range of a given variable. The first approach is, therefore, to remove all features with such values. Nevertheless, as stated in \cite{dataset}, depending on the number of jets of the event, some features can be undefined. Thus, we filter the raw data by the jet number and then eliminate all meaning-less values, which, in most cases, are all the events of a given feature. Depending on the topology of the event, if the event is not too far from the expected topology, a feature representing the estimated mass of the Higgs boson candidate could be also defined. Given that this last feature is thought to be meaningful if defined, we also filter the dataset between valid estimated mass or not.

Doing a exploratory analysis of every chunk of data, we find that a lot of values could be defined as outliers following the outer fence approach presented in \cite{tukey77}. This approach is based in determining the inter-quartile range $IQR$ of the data separated by features, i.e. the difference between the third and first quartiles, and then setting expected minimum and maximum values from that. All the data points that lay outside this range are considered outliers. Then, the outer fence defines the following limits:
\begin{equation}
    \min = Q_1-3IQR \quad ; \quad \min = Q_3+3IQR
\end{equation}

% Put plot before and after eliminating outliers

It turns out that if we explore the outliers by features, they represent a very low percentage by mere definition. However, if we eliminate an event that has outliers, we would end up with only around $20\%$ of the original dataset. In order to avoid introducing fake data, we do not change the value of an outlier candidate such as the median or the nearest quartile.

%Due to the fact that we want to avoid changing the value of an outlier for a statistical metric such as the median or the nearest quartile to avoid introducing fake data that could degrade the performance, we will not treat the outlier candidates.
% in order to avoid introducing fake data that would degrade the performance, we will not change the value of an outlier candidate 

%Nevertheless, we are aware that they can have a bad influence in the performance of our model. 

 For linear models (e.g. linear regression or logistic regression), multicolinearity can yield to solutions that are wildly varying and possibly numerically unstable. Therefore, as part of the data analysis, we compute the similarity of the features using the Pearson correlation coefficient by measuring the linear correlation between the variables. Unfortunately, although 3 of the features (DER\_sum\_pt, PRI\_met\_sumet, PRI\_jet\_all\_pt \cite{dataset}) are highly correlated between them (with value $\geq$ 0.96), removing one or more of these features does not show any significant improvement on our results. This shows us the difference between correlation and causation.

Another approach regarding the data preprocessing is to build polynomials with cross terms. That is because it is very likely that the theoretical value of the existence of the Higgs boson decaying into tau particles is based on combining some parameters, as in almost all physics models. Therefore, we create powers of a certain degree for every feature, as usual in feature expansion, but we also incorporate sums, products and squares of products for cross terms of the generated polynomial. This ends up with a lot of new features which results in better models.

%We also consider to apply dimensionality reduction with Principal Component Analysis (PCA) in our first stage models. Nevertheless, given that the polynomial with cross terms obtains better results and does not seem to overfit, even if it has some hundreds of features, we no longer apply this transformation.

Applying dimensionality reduction with Principal Component Analysis (PCA) to our data might also improve the results of our model. Nevertheless, given that the polynomial with cross terms obtains better results and does not seem to overfit, even if it has some hundreds of features, it is no longer needed to apply this transformation.

\subsection{Model}
\label{sec:model}

Different models are tested to tackle our classification problem using different algorithms to optimize a loss function: Gradient Descent (GD), Stochastic Gradient Descent (SGD), least squares, ridge regression and logistic regression.

For our final model, we use logistic regression because it is optimized for binary classification and we add a penalty factor $\lambda$ in order to avoid over-fitting. 

We should remark that we also add a little modification to the original loss function for logistic regression in order to avoid computational overflow. The sigmoid function (Equation \ref{eq:sigmoid}) only takes values of 0 or 1 in the limits to $\pm \infty$. However, due to limitations of float type, when applying the sigmoid to very large or small numbers we get an approximation of 1 or 0. That produces a problem when computing the logarithm in the loss function. Therefore, we add a threshold in the input of the function in order to ensure a floating-point results and the loss is computed following Equation \ref{eq:losslog}.

\begin{equation}
    \sigma(t) = \frac{1}{1 + \mathrm{e}^{-t}}
    \label{eq:sigmoid}
\end{equation}

\begin{align}
    \mathcal{L}(w) &= - \sum_{n=1}^{N} \lbrack y_n \log (\sigma (\max\{x_n^T w, -10\})) + \\&+ (1 - y_n) \log (1- \sigma (\min\{ x_n^T, 10\})) \rbrack
    \label{eq:losslog}
\end{align}

%\begin{itemize}
%    \item[--] \textbf{Gradient Descent (GD):} the cost function is optimized by iteratively updating the weights in the opposite direction of the gradient.
%    \item[--] \textbf{Stochastic Gradient Descent (SGD):} the same idea as the GD is applied. An unbiased estimation of the gradient is computed from a randomly selected subset of samples.
%   \item[--] \textbf{Least squares:} the model is trained by solving a linear system of equations which minimizes the mean-squared error (MSE) cost function.
%    \item[--] \textbf{Ridge regression:} the model is trained by solving a linear system of equations which minimizes the MSE cost function and regularization is added to avoid overfitting.
%    \item[--] \textbf{Logistic regression:} the model is used for binary classification (the loss function is optimized for that purpose), where the predictions are transformed into probabilities by applying the logistic function. 
    %in order to avoid the values of predictions contribute to the error as if using the MSE loss.
%\end{itemize}

\subsection{Cross-validation}
\label{sec:cv}

In order to tune the hyper-parameters of our model, k-fold cross-validation is used with $k=10$ folds. The train dataset is randomly chopped into 10 equal sized sets. We tuned the values of the polynomial degree, in both cases where it was simple or with cross terms, and the penalty factor $\lambda$. The best parameters are then selected by performing grid search for all parameters and choosing those that yield to the highest average accuracy across folds. This algorithm is used separately for each of the 4 partitions of the train dataset according to the number of JET feature and each of them is also divided in 2 depending on the presence of the mass value, as explained in Section \ref{sec:data-cleaning}. That makes a total of 8 partitions with different hyper-parameters to tune.

The accuracies for the best parameters obtained with cross-validation are illustrated in Table \ref{tab:cv}.

\begin{table}[htbp]
  \centering
  \begin{tabular}[c]{|c|c|c|c|c|}
    \hline
    JET & With mass & Best degree & Best $\lambda$ & Accuracy\\
    \hline
    0 & True & $7$ & $10^{-5}$ & $0.797 \pm 0.004$\\
    0 & False & $12$ & $10^{-5}$ & $0.986 \pm 0.003$\\
    1 & True & $9$ & $10^{-4}$ & $0.739 \pm 0.005$ \\
    1 & False & $10$ & $10^{-6}$ & $0.977 \pm 0.007$ \\
    2 & True & $8$ & $10^{-5}$ & $0.557 \pm 0.006$ \\
    2 & False & $9$ & $10^{-10}$ & $0.892 \pm 0.016$ \\
    3 & True & $9$ & $10^{-4}$ & $0.832 \pm 0.008$ \\
    3 & False & $9$ & $10^{-2}$ & $0.975 \pm 0.011$ \\
    \hline
  \end{tabular}
  \caption{Best parameters obtained with cross-validation}
  \label{tab:cv}
\end{table}


\section{Experiments and results}
\label{sec:results}

In Table \ref{tab:models} we can see the different performances of some of the models tested. We added a random guess model as a reference to see the improvement of each model. All those models have a previous data cleaning that only consisted in removing the features with meaning-less values.

\begin{itemize}
    \item[--] \textbf{Model A:} Random guess (reference)
    \item[--] \textbf{Model B:} Gradient Descent (MSE loss function)
    \item[--] \textbf{Model C:} Stochastic Gradient Descent (MSE loss function)
    \item[--] \textbf{Model D:} Least squares
    \item[--] \textbf{Model E:} Ridge regression
    \item[--] \textbf{Model F:} Regularized logistic regression
    \item[--] \textbf{Model G:} Regularized logistic regression with JET and mass filtering and adaptive learning rate
\end{itemize}

\begin{table}[htbp]
  \centering
  \begin{tabular}[c]{|c|c|}
    \hline
    Model & Accuracy \\
    \hline
    A & $0.500 \pm 0.001$\\
    B & $0.706 \pm 0.002$\\
    C & $0.709 \pm 0.003$\\
    D & $0.774 \pm 0.001$\\
    E & $0.767 \pm 0.001$\\
    F & $0.743 \pm 0.001$\\
    G & $0.823 \pm 0.004$\\
    \hline
  \end{tabular}
  \caption{Performance with different models}
  \label{tab:models}
\end{table}

As we can see, the best model in terms of the obtained accuracy, is the regularized logistic regression with Gradient Descent optimization and adaptive learning rate. Regarding the approach to update the learning rate, we decay by a $\gamma$ factor after a given number of iterations. This is inspired by the Step Learning rate Scheduler of Pytorch\cite{pytorch}.

The decaying factor of the learning rate is set to $\gamma=0.1$, which is the default value provided by the Pytorch library, and it is updated every 2000 iterations. This last value is set based on the evolution of the loss function across iterations to avoid having a very large step size at the first iterations or very few improvements in the last ones.

\section{Conclusions}
\label{sec:conclusions}
With this project, we show the importance of understanding the data and doing a proper preprocessing before applying machine learning algorithms to build a statistical model. Our problem has a lot of non valid values which are finally treated by filtering the data of certain events that condition the meaningfulness of others. In this dataset, the number of outliers is bigger than in other problems and, given that these are often non-overlapping, we are not able to eliminate them.

Regarding the best obtained model, it is trivial to see that the logistic loss makes much more sense in a binary classification framework than other usual metrics like the mean-square error, which do not penalize the miss-classification but only the distance to the ground-truth. The source code of this project is available on GitHub\footnote{\url{https://github.com/Barbany/ML-Project1}}.


\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}
